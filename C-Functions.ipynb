{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "**Topics**: Introducing functions and modules in Python. Basic introduction to pandas for data analysis, focusing on importing data and initial data exploration.\n",
    "\n",
    "## Functions\n",
    "A function is a grouping of code that we assign a name and can pass specific data to (arguments) and return data from (return value)\n",
    "\n",
    "We use functions for a few things:\n",
    "* Reduce dupliation in code - use the same function in multiple places in your code.\n",
    "* Simplify code - breaking down complex code into smaller, separate, problems make the entire code more managable and maintainable. \n",
    "* Readability - named functions say specifically what they're going to do, so our program is less cluttered and easier to follow. \n",
    "\n",
    "A note about programming in notebooks like this... breaking code up into cells helps to organize it like a function might in a script.  And the most improvemint I've seen in notbooks on duplicated code is by organizing data into dictionaries and using loops to work on each group of data one at a time.  Functions are very important when writing scripts and larger programs, but a little less so in notebooks, except that when we import libraries, we call functions in the librar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Format of a Function\n",
    "Here's how we define a function:\n",
    "\n",
    "    def function_name(arg1, arg2, ...):\n",
    "        '''function description in tripple quoted block of text.\n",
    "        This is not mandatory, but is good practice.'''\n",
    "        function\n",
    "        code\n",
    "        here\n",
    "        some_value = foo\n",
    "        return some_value\n",
    "\n",
    "We can only return one object, but because that object can be a collection like a list or dictionary, we can bundle things to pass them all out.  Examples:\n",
    "\n",
    "    return {'a': 'dictionary', 'is': 'okay}\n",
    "    return 'this', 'will', 'return', 'a', 'tuple'\n",
    "    x = ['a', 'list', 'works', 'too']\n",
    "    return x\n",
    "\n",
    "Here's an example returning a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats_on_numbers(list_of_numbers):\n",
    "    sum_of_numbers = sum(list_of_numbers)\n",
    "    count_of_numbers = len(list_of_numbers)\n",
    "    average_of_numbers = sum_of_numbers / count_of_numbers\n",
    "    return sum_of_numbers, count_of_numbers, average_of_numbers  # This is a tuple.  The () around it are implied\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "num_sum, num_count, num_avg = compute_stats_on_numbers(numbers)\n",
    "\n",
    "print(f'The function says - Sum: {num_sum}, Count: {num_count}, Average: {num_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "This is a new concept for us - there are certain places where variables can be defined that they will be unaccessible externally.  The variables have a specific scope in which they can be used. \n",
    "* **Global** - variables defined outside of functinos, classes, etc, in your program are accessible from everywhere. However, it's bad practice to use global variables from inside of functions as it makes it hard to follow what data is used by the function.  Side effects can be introduced.\n",
    "* **Functions** - variables defined inside of functions are not visible outside of the function.  This means we don't neeed to worry about accidentally using a variable from a(nother) function when we don't mean to. \n",
    "* **Classes/Objects** - objects (instances of a class) have thier own variables/properties and functions that aren't accessible externally.\n",
    "* **Modules** - modules imported like, \"import pandas\", have their own scope inside of \"pandas\" that we access via the module name, like \"pandas.DataFrame\".  If we were to do \"from pandas import *\", then all things in the pandas namespace would be populated into our global namespace and we could directly access DataFrame.  This can introduce problems, e.g., if multiple modules have things with the same name inside of them. It's better to import specific things to our global namespace if wanted... \"from pandas import DataFrame\" will only add the DataFrame class to our global namespace.\n",
    "* And a few other places.  Try except blocks, inside of list comprehensions, etc.\n",
    "\n",
    "What this means to us with regard to functions is that we should pass data the function needs in as arguments, create any variables in the function that we need without worrying about them polluting the namespace of our greater program, and then return the important data from the function with a return call.\n",
    "\n",
    "#### *Exercise*\n",
    "Let's investigate the nuances of global and local variables in a function.  Do this:\n",
    "\n",
    "* Run the cell below and not the values of x inside and outside of the function.\n",
    "* Uncomment the x=3 line and see what changes\n",
    "* Uncomment the global declaration in the function ans run it again to see what changes.\n",
    "\n",
    "At first, x only exists in the global namespace, so when we call print, python finds it there. \n",
    "\n",
    "When we uncomment the x=3, we define x in the function's local namespace, so that is what gets printed.  The function's namespace will always be used before the global namespace. Note that we don't overwrite the global namespace x value when we set x in the function. \n",
    "\n",
    "When we uncomment the global line, we are declaring that the x in the function is in the global namespace, so when we set x=3, we are able to change the global x.  There are times when this is useful, but in general we should try not to do this because it makes it harder to debug code and hides interaction between stuff.  We should pass data the function needs in as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global x is 2\n",
      "x in func is: 2\n",
      "foo in func is: 2\n",
      "now global x is 2\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "def print_a_value(foo):\n",
    "    # global x\n",
    "    # x = 3\n",
    "    print('x in func is:', x)\n",
    "    print('foo in func is:', foo)\n",
    "print('global x is', x)\n",
    "print_a_value(x)\n",
    "print('now global x is', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Arguments\n",
    "When we define a function with multiple arguments like this:\n",
    "\n",
    "    def do_the_thing(pos1, pos2, pos3, ..., posN):\n",
    "\n",
    "We must pass the function N arguments with positions corresponding with the function definition.\n",
    "\n",
    "    return_value = do_the_thing('stuff1', 'stuff2', 'stuff3', ..., 'stuffN')\n",
    "\n",
    "### Optional Arguments\n",
    "We can also set default values for arguments, startning with argument N and working backward.  We cannot set a default values for pos1 but not for pos2.\n",
    "\n",
    "    def do_the_thing(arg1, arg2, arg3=False, arg4=True)\n",
    "\n",
    "In this case, we must pass arg1 and arg2, but we can omit arg3 and arg4 if we don't need them. \n",
    "\n",
    "Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def generate_random_data(num_rows, num_cols, to_dataframe=False):\n",
    "    '''Thes function accepts a number of rows and number of columns and\n",
    "    generates a table of random data.  If to_dataframe is True, it will\n",
    "    return a pandas DataFrame.  Otherwise, it will return a list of dictionaries.'''\n",
    "    data = [\n",
    "        {f'col_{j+1}': random.random() for j in range(num_cols)}\n",
    "        for i in range(num_rows)\n",
    "    ]\n",
    "    \n",
    "    if to_dataframe:\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "random_data_list = generate_random_data(5, 3)  # not necessary to specify to_dataframe=False\n",
    "random_data_df = generate_random_data(5, 3, to_dataframe=True)\n",
    "\n",
    "print(random_data_list)\n",
    "print(random_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two lines from Example usage, the fist line skipps passing to_dataframe because the default value is acceptable.\n",
    "\n",
    "#### *Exercise*\n",
    "Write a functon called **prompt_user** that accepts two arguments, **choices**, and **num_tries**.  \n",
    "\n",
    "It should ask the user to chose one of the choices, and then try num_tries times to let them type in a choice. If what they type in doesn't match any choices, then have them try again.  If they don't do it successfully in num_tries, then return False. If they do chose one, then return that choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_user(...):\n",
    "    ...\n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it!\n",
    "choices = ['red', 'green', 'blue']\n",
    "choice = prompt_user(choices, 2)\n",
    "if choice:\n",
    "    print(f'You chose {choice}')\n",
    "else:\n",
    "    print('You did not choose a valid option')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Arguments\n",
    "Finally, some functions have lots of optional arguments.  Often with default values of False for skipping some functionality in the function, or they could have sane defaults like a function to read_a_csv_file might have a default header_row=0 to use the first row of the file as the column headers.  You'd only change it when you call the function if you have padding rows at the top of your file. \n",
    "\n",
    "Let's look at some examples of the pandas read_excel function with different combinations of arguments given.  Compare to the function documentation here: https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n",
    "\n",
    "If we have an excel file with multiple sheets, but we want specifically to load the data from sheet2, we can do:\n",
    "\n",
    "    df = pd.read_excel('data.xlsx', sheet_name='Sheet2')\n",
    "\n",
    "Or if it is only one sheet, but we want to load specific columns and skip the top two rows in the file:\n",
    "\n",
    "    df = pd.read_excel('data.xlsx', usecols=['A', 'C', 'E'], skip_rows=2)\n",
    "\n",
    "\n",
    "### Arbitrary Arguments\n",
    "We won't get into this, but look into *args and **kwargs.  You can make a funcation accept any arguments.  An example use for this is cerating your own version of the print function:\n",
    "\n",
    "    DEBUG = True\n",
    "\n",
    "    def debug_print(*args, **kwargs):\n",
    "        if DEBUG:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "#### *Exercise*\n",
    "Let's make a \"greeting_generator\" function that accepts a few arguments and returns a string with the generated greeting message. Arguments:\n",
    "* name - required argument, so it should not have a default value. \n",
    "* greeting - optional argument with a default value of \"Hello\". \n",
    "* punctuation - optional argument with default value of \"!\".\n",
    "* height_in_feet - optional argument with default value of False. If given, we append the string with something witty about the user's height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting_generator(...):\n",
    "    ...\n",
    "    return greeting\n",
    "\n",
    "# Let's test it!\n",
    "print(greeting_generator('Bob'))\n",
    "print(greeting_generator('Alice', 'Good morning'))\n",
    "print(greeting_generator('Charlie', height=6))\n",
    "print(greeting_generator('Diane', punctuation='!', height=4))\n",
    "print(greeting_generator('Eve', 'Good night', '!', 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "We've used a few modules so far.  Here's a summary of some common modules:\n",
    "\n",
    "* **Data analysis and math**\n",
    "  * pandas - Manipulate structured data in DataFrames.  Built on numpy.  Sort of like excel but less tedious. \n",
    "  * matplotlib - Data visualization tool.  We use it to generate axis and subplots for more interesting plots. \n",
    "  * seaborn - Advanced data visualization and analysis tools.  \n",
    "  * numpy - Work with arrays of data.  Vectorize data operations for performance. \n",
    "  * math - Trig functions, sqrt, etc. Used for individual values.  x = math.tan(y)\n",
    "  * datetime - Convert string data to datetime objects and vice-versa.  Perform time operations, like adding hours, days, etc. \n",
    "* **OS and file handing**\n",
    "  * sys - Access environment variables, \"exit\", get system information.\n",
    "  * os - List files (os.listdir), modify permissions, filesystem stats, user account stuff. \n",
    "  * shutil - Helper funcions for moving and copying files, few other things.\n",
    "  * tar, zip - Open or create zip and tar archive files with these. tar is more common on linux systems. \n",
    "  * subprocess - Execute programs or commands outside of python.\n",
    "* **Data encapsulation and databases...**\n",
    "  * json - Structured text format of the web and many things. Use format=\"pretty\"\n",
    "  * yaml - Like json, but more friedly for humans to edit the files.  More flexible allowing in line comments in the file.\n",
    "  * pickle - pickle and unpickle nearly any python object to save in a file. \n",
    "  * sqlite3 - file based database\n",
    "  * mysql - open source mysql database connections...\n",
    "  * pyodbc - odbc based database connections\n",
    "* **Network stuff**\n",
    "  * requests - talk to web servers\n",
    "\n",
    "### Conventional short names\n",
    "Some modules have accepeted conventions for short names to reduce typing and whatnot. Here are a few common ones:\n",
    "* import pandas as pd\n",
    "* import numpy as np\n",
    "* import subprosess as sp\n",
    "\n",
    "### Importing modules or parts of modules\n",
    "\n",
    "We can import modules and access their tools my module name like:\n",
    "\n",
    "    import math\n",
    "    x = math.sqrt(50)\n",
    "\n",
    "Or we can import specific components of a module:\n",
    "\n",
    "    from math import sqrt,cos,sin\n",
    "    x = cos(30)\n",
    "\n",
    "You can import all things from a module into your global namespace, but it's discouraged.  What if you import two modules that have components with the same names in them?\n",
    "\n",
    "    from math import *\n",
    "\n",
    "When we do \"import math\", all of the variables and functions in that module are protected in a private namespace that we access via math.something().  \n",
    "\n",
    "#### *Exercise*\n",
    "Let's start getting more familiar with pandas.\n",
    "* Read through this: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "* Import the pandas module and use dir(pd) to see what functionality is built into it.  Or if you have python running in a terminal, type 'pd.' and hit the tab key to show a list of functions built into it. \n",
    "* Create an empty pandas dataframe and do the same as above to see what functionality is built into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()  # An empty dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Pandas Experiment\n",
    "Nasa has this list of meteorites that we can use.  Let's see what we can learn about this data.  We'll import it and generate some plots to better understand it.\n",
    "\n",
    "First thing is importing.  We use requests to query the url, get the json data, and convert it to a dataframe.  A few useful funtions for viewing data in a dataframe are .head(), .tail(), and .info().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "meteorites = requests.get('https://data.nasa.gov/resource/y77d-th95.json').json()\n",
    "mets = pd.DataFrame(meteorites)\n",
    "mets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have geolocation, mass, and datetime info for each meteor.  Let's try graphing mass per date to see if there's any obvious trend. \n",
    "\n",
    "We need each column to be the correct datatype before we can generate a plot.\n",
    "* To simplify the datetime 'year' column, I use a string operation to split it on the 'T' ang take just the year, month and day.  Then we can use pd.to_datetime do convert it to a datetime object by passing in the format to use to convert it. \n",
    "* We need the mass to be a numeric value so we overwrite the column with itself converted using pd.to_numeric.  Similarly, there ar pd.to_int, pd.to_float, pd.to_string operatoins that we might want to use in other cases. \n",
    "\n",
    "Finally, pandas has a built in plot function that can generate a bunch of different graph types. Setting 'logy' says to graph the y axis in log scale. Try setting it and see what happens to the data points and y axis scale. \n",
    "\n",
    "https://www.w3schools.com/python/gloss_python_date_format_codes.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the meteorites by date and mass\n",
    "mets['ymd'] = mets['year'].str.split('T').str[0]\n",
    "mets['ymd'] = pd.to_datetime(mets['ymd'], format='%Y-%m-%d', errors='coerce')\n",
    "mets['mass'] = pd.to_numeric(mets['mass'])\n",
    "mets.plot.scatter(x='ymd', y='mass', logy=True, title='Meteorite Mass by Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's neat, but doesn't show much of a trend except that we probably have better records of meteorites found since the late 1800s.  Maybe it would be interesting to see where on earth we are finding the meteorites.  \n",
    "\n",
    "Let's plot them on a map of the earth.  First thing for that is to get a map of the earth.  We can use some geopandas stuff for that.  Below, \"world\" is a dataframe with rows for each landmass on a map.  Try printing world.head() to see some of the actual data.  \n",
    "\n",
    "https://geopandas.org/en/stable/docs/user_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from geodatasets import get_path\n",
    "\n",
    "path = get_path(\"naturalearth.land\")\n",
    "world = geopandas.read_file(path)\n",
    "world.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets put it together!  Plotting can get complicated fast... I think that's a weakness of this stuff in python with the pandas, matplotlib libraries, but it's very powerful at least. \n",
    "\n",
    "To combine plots, we initialize an axis that we pass to the .plot function when we call it for the world and our meteorite dataframes so that the can draw themselves on the same graph. 'ax=ax' looks a littele weird.  We'r passing a variable named ax to an argument with the same name.  It's just sort of convention to do it this way.  Maybe it would be better to use axis for the variable name and pass that to the plot function. \n",
    "\n",
    "We need to convert our reclong and reclat (longitude and latitude) to numeric values to plot them, so use call .astype(float) to do a type conversion from string.\n",
    "\n",
    "You can change the colormap - if you put in a bad value, it'll print a bunch you can try in the error message.  And the norm= is to convert the mass to log scale here so that we get nice colors for all of the meteorite masses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))           # create a figure and axis\n",
    "\n",
    "world.plot(ax=ax)                                # plot the world on the axis\n",
    "\n",
    "mets['reclong'] = mets['reclong'].astype(float)  # convert reclong column from string to float\n",
    "mets['reclat'] = mets['reclat'].astype(float)    # convert reclat column from string to float\n",
    "\n",
    "# plot the meteorites on the same axis\n",
    "mets.plot(x=\"reclong\", y=\"reclat\", kind=\"scatter\", \n",
    "        c=\"mass\", colormap=\"Accent_r\", \n",
    "        title=f\"Meteors around the world!\", \n",
    "        ax=ax, norm=matplotlib.colors.LogNorm())\n",
    "\n",
    "ax.grid(True)                                   # turn on the grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's neat.  I extected to see more concentration around the equator.  We might need to normalize for population density (people who could see and find an asteroid to report on) to get an idea of were most asteroids actually fall.  There is a larger version of this dataset at: \n",
    "\n",
    "We'll look at more actual numerical analysis stuff in Week 4.\n",
    "\n",
    "#### *Exercise*\n",
    "Open a new notebook and download the dataset for observed meteors from here: https://www.kaggle.com/datasets/ramjasmaurya/fireballsbolides-1988-april-2022.  Save it in the same directory as your notebook.  If you're working in Google Colab, you should be able to go to File -> Locate notebook in Drive, and then upload the dataset csv file to the same directory in Google drive. \n",
    "\n",
    "Copy over the following code to get started, and use the above example for the meteorites to make a couple of graphs for this new dataset.\n",
    "* radiated energy vs time\n",
    "* altitude vs radiated energy\n",
    "* try using .corr(numeric_only=True) on the dataframe to see which numeric columns have the strongest correlation.   What can we say about these boloids when they enter our atmosphere?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = 'nasas fireballs.csv'\n",
    "nfb = pd.read_csv(csv_file_name)\n",
    "# nfb.head() # Uncomment to see the first few rows of the data\n",
    "# nfb.info() # Uncomment to see the column names and data types of each column\n",
    "\n",
    "# the date/time ... column is a string that we want to convert to a datetime object\n",
    "# we're creating a 'date' column for this.  We could also overwrite the existing column\n",
    "# nfb['date'] = pd.to_datetime(nfb['date/time for peak brightness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### *Exercise*\n",
    "Go through some of these datasets and find something that looks interesting to you that we can work on next week.\n",
    "\n",
    "* https://github.com/jdorfman/awesome-json-datasets - we can direcly query these using requests and the url as we did for the meteorite data. \n",
    "* https://catalog.data.gov/dataset/\n",
    "* https://data.fivethirtyeight.com/ - they have zip files with csv data\n",
    "* https://www.kaggle.com/datasets - click all data sets and you'll see loads of stuff.  looks like they have large csv files to download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Turtle Challenge\n",
    "This week, we can use functions to isolate complex operations into little chunks that are used by other code to perform complex behavior with simple, readable, code.\n",
    "  \n",
    "#### *Exercise*:\n",
    "Streamline your turtle code from last week by moving the functionality to draw arbitrary shapes into a function.  The function should take arguments for numbers of sides and size and will be called from the ret of your code from last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
